---
title: "Areal Data and Proximity"
subtitle: "HES 505 Fall 2024: Session 19"
author: "Carolyn Koehn"
execute: 
  eval: true
format: 
  revealjs:
    theme: mytheme.scss
    slide-number: true
    show-slide-number: print
    self-contained: true  
---

```{r}
#| include: false
library(terra)
library(tmap)
library(sf)
library(tidyverse)
library(spatstat)  # Used for the dirichlet tesselation function
library(sp)
```

# Objectives {background="#0033A0"}

By the end of today you should be able to:

* Describe and implement statistical approaches to interpolation

* Describe the case for identifying neighbors with areal data

* Implement contiguity-based neighborhood detection approaches

* Implement graph-based neighborhood detection approaches

# Statistical Interpolation {background="#9F281A"}

## Statistical Interpolation

::: columns
::: {.column width="40%"}
* Previous methods predict $z$ as a (weighted) function of distance

* Treat the observations as perfect (no error)

* If we imagine that $z$ is the outcome of some spatial process such that:

:::
:::{.column width="60%"}

```{r}
#| cache: true
aq <- read_csv("../data/ad_viz_plotval_data_PM25_2024_ID.csv") %>% 
  st_as_sf(., coords = c("Site Longitude", "Site Latitude"), crs = "EPSG:4326") %>% 
  st_transform(., crs = "EPSG:8826") %>% 
  mutate(date = as_date(parse_datetime(Date, "%m/%d/%Y"))) %>% 
  filter(., date >= 2024-07-01) %>% 
  filter(., date > "2024-07-01" & date < "2024-07-31")
aq.sum <- aq %>% 
  group_by(., `Site ID`) %>% 
  summarise(., meanpm25 = mean(`Daily AQI Value`))

id.cty <- tigris::counties(state="ID", progress_bar=FALSE) %>% 
  st_transform(., crs=st_crs(aq.sum))
tm_shape(id.cty) +
  tm_polygons(fill="gray") +
tm_shape(aq.sum) +
  tm_dots(col = "meanpm25", size=2) 
```
:::
:::

:::{.notes}
Reminder of inverse distance weighting - z (outcome) is just a function of distance
:::

## Trend Surface Modeling

- Basically a regression on the coordinates of your data points

- Coefficients apply to the coordinates and their interaction

- Relies on different functional forms

:::{.notes}
- Still no covariates, just prediction based on coordinates
- Coefficients describe how changes in x, y, or their interaction change z
- Draw out regression line and equation (z = b1x + b2y + b0)
:::

## 0th Order Trend Surface

::: columns
::: {.column width="60%"}
```{r echo=FALSE}
#set up interpolation grid
# Create an empty grid where n is the total number of cells
bbox <- st_bbox(id.cty)
grd <- st_make_grid(id.cty, n=150, 
                    what = "centers") %>%
  st_as_sf() %>%
  mutate(X = st_coordinates(.)[, 1], 
         Y = st_coordinates(.)[, 2])

# Define the polynomial equation
f.0  <- as.formula(meanpm25 ~ 1)

# Run the regression model
lm.0 <- lm( f.0 , data=aq.sum)

# Use the regression model output to interpolate the surface
grd$var1.pred <- predict(lm.0, newdata = grd)
# Use data.frame without geometry to convert to raster
dat.0th <- st_drop_geometry(grd)

# Convert to raster object to take advantage of rasterVis' imaging
# environment
r   <- rast(dat.0th, crs = crs(grd))
r.m <- mask(r, st_as_sf(id.cty))

tm_shape(r.m) + 
  tm_raster( title="Predicted air quality") +
  tm_shape(aq.sum) + 
  tm_dots(size=0.2) +
  tm_legend(legend.outside=TRUE)
```
:::
:::{.column width="40%"}

* Simplest form of trend surface

* $Z=a$ where $a$ is the mean value of air quality

* Result is a simple horizontal surface where all values are the same.

:::
:::

:::{.notes}
Intercept only model (mean when all coefficients are 0)
:::

## 0th order trend surface

```{r}
#| echo: true
#| eval: false
#set up interpolation grid
# Create an empty grid where n is the total number of cells
bbox <- st_bbox(id.cty)
grd <- st_make_grid(id.cty, n=150, 
                    what = "centers") %>%
  st_as_sf() %>%
  mutate(X = st_coordinates(.)[, 1], 
         Y = st_coordinates(.)[, 2])

# Define the polynomial equation
f.0  <- as.formula(meanpm25 ~ 1)

# Run the regression model
lm.0 <- lm( f.0 , data=aq.sum)

# Use the regression model output to interpolate the surface
grd$var0.pred <- predict(lm.0, newdata = grd)
# Use data.frame without geometry to convert to raster
dat.0th <- grd %>%
  select(X, Y, var0.pred) %>%
  st_drop_geometry()

# Convert to raster object to take advantage of rasterVis' imaging
# environment
r   <- rast(dat.0th, crs = crs(grd))
r.m <- mask(r, st_as_sf(id.cty))

tm_shape(r.m) + 
  tm_raster( title="Predicted air quality") +
  tm_shape(aq.sum) + 
  tm_dots(size=0.2) +
  tm_legend(legend.outside=TRUE)
```

## 1st Order Trend Surface

::: columns
::: {.column width="40%"}

* Creates a slanted surface

* $Z = a + bX + cY$

* X and Y are the coordinate pairs

:::
:::{.column width="60%"}

```{r echo=FALSE}

# Define the polynomial equation
f.1  <- as.formula(meanpm25 ~ X + Y)

aq.sum$X <- st_coordinates(aq.sum)[,1]
aq.sum$Y <- st_coordinates(aq.sum)[,2]

# Run the regression model
lm.1 <- lm( f.1 , data=aq.sum)

# Use the regression model output to interpolate the surface
grd$var1.pred <- predict(lm.1, newdata = grd)
# Use data.frame without geometry to convert to raster
dat.1st <- grd %>%
  select(X, Y, var1.pred) %>%
  st_drop_geometry()

# Convert to raster object to take advantage of rasterVis' imaging
# environment
r   <- rast(dat.1st, crs = crs(grd))
r.m <- mask(r, st_as_sf(id.cty))

tm_shape(r.m) + 
  tm_raster( title="Predicted air quality") +
  tm_shape(aq.sum) + 
  tm_dots(size=0.2) +
  tm_legend(legend.outside=TRUE)
```
:::
:::

## 1st Order Trend Surface

```{r}
#| echo: true
#| eval: false

# Define the polynomial equation
f.1  <- as.formula(meanpm25 ~ X + Y)

aq.sum$X <- st_coordinates(aq.sum)[,1]
aq.sum$Y <- st_coordinates(aq.sum)[,2]

# Run the regression model
lm.1 <- lm( f.1 , data=aq.sum)

# Use the regression model output to interpolate the surface
grd$var1.pred <- predict(lm.1, newdata = grd)
# Use data.frame without geometry to convert to raster
dat.1st <- grd %>%
  select(X, Y, var1.pred) %>%
  st_drop_geometry()

# Convert to raster object to take advantage of rasterVis' imaging
# environment
r   <- rast(dat.1st, crs = crs(grd))
r.m <- mask(r, st_as_sf(id.cty))

tm_shape(r.m) + 
  tm_raster( title="Predicted air quality") +
  tm_shape(aq.sum) + 
  tm_dots(size=0.2) +
  tm_legend(legend.outside=TRUE)
```

## 2nd Order Trend Surfaces

* Produces a parabolic surface

* $Z = a + bX + cY + dX^2 + eY^2 + fXY$

* Highlights the interaction of both directions

```{r}
# Define the 1st order polynomial equation
f.2 <- as.formula(meanpm25 ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))
 
# Run the regression model
lm.2 <- lm( f.2, data=aq.sum)

# Use the regression model output to interpolate the surface
grd$var2.pred <- predict(lm.2, newdata = grd)
# Use data.frame without geometry to convert to raster
dat.2nd <- grd %>%
  select(X, Y, var2.pred) %>%
  st_drop_geometry()

r   <- rast(dat.2nd, crs = crs(grd))
r.m <- mask(r, st_as_sf(id.cty))

tm_shape(r.m) + tm_raster(n=10, title="Predicted air quality") +
  tm_shape(aq.sum) + 
  tm_dots(size=0.2) +
  tm_legend(legend.outside=TRUE)
```

:::{.notes}
We have an east-west trend and a north-south trend
:::

## 2nd Order Trend Surfaces

```{r}
#| echo: true
#| eval: false
# Define the 1st order polynomial equation
f.2 <- as.formula(meanpm25 ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))
 
# Run the regression model
lm.2 <- lm( f.2, data=aq.sum)

# Use the regression model output to interpolate the surface
grd$var2.pred <- predict(lm.2, newdata = grd)
# Use data.frame without geometry to convert to raster
dat.2nd <- grd %>%
  select(X, Y, var2.pred) %>%
  st_drop_geometry()

r   <- rast(dat.2nd, crs = crs(grd))
r.m <- mask(r, st_as_sf(id.cty))

tm_shape(r.m) + tm_raster(n=10, title="Predicted air quality") +
  tm_shape(aq.sum) + 
  tm_dots(size=0.2) +
  tm_legend(legend.outside=TRUE)
```


## Kriging
::: {style="font-size: 0.8em"}
* Previous methods predict $z$ as a (weighted) function of distance

* Treat the observations as perfect (no error)

* If we imagine that $z$ is the outcome of some spatial process such that:

$$
\begin{equation}
z(\mathbf{x}) = \mu(\mathbf{x}) + \epsilon(\mathbf{x})
\end{equation}
$$

then any observed value of $z$ is some function of the process ($\mu(\mathbf{x})$) and some error ($\epsilon(\mathbf{x})$)

* Kriging exploits autocorrelation in $\epsilon(\mathbf{x})$ to identify the trend and interpolate accordingly
:::

## Autocorrelation

* __Correlation__ the tendency for two variables to be related

* __Autocorrelation__ the tendency for observations that are closer (in space or time) to be correlated

* __Positive autocorrelation__ neighboring observations have $\epsilon$ with the same sign

* __Negative autocorrelation__ neighboring observations have $\epsilon$ with a different sign (rare in geography)

## Ordinary Kriging

* Assumes that the deterministic part of the process ($\mu(\mathbf{x})$) is an unknown constant ($\mu$)

$$
\begin{equation}
z(\mathbf{x}) = \mu + \epsilon(\mathbf{x})
\end{equation}
$$

## Steps for Ordinary Kriging

+ Removing any **spatial trend** in the data (if present).
+ Computing the **experimental variogram**, $\gamma$, which is a measure of spatial autocorrelation.
+ Defining an **experimental variogram model** that best characterizes the spatial autocorrelation in the data.
+ Interpolating the surface using the experimental variogram.
+ Adding the kriged interpolated surface to the trend interpolated surface to produce the final output.

:::{.notes}
Figure out the part of the spatial trend that's not related to X/Y then merge them back together
:::

## Removing Spatial Trend

* Mean and variance need to be constant across study area

* Trend surfaces indicate that is not the case

* Need to remove that trend

```{r}
#| echo: true

f.2 <- as.formula(meanpm25 ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))
 
# Run the regression model
lm.2 <- lm( f.2, data=aq.sum)

# Copy the residuals to the point object
aq.sum$res <- lm.2$residuals
```

:::{.notes}
Clarify what residuals are -- distance of points from line in lm
:::

## Removing the trend

```{r}
tm_shape(id.cty) +
  tm_polygons(fill="gray") +
tm_shape(aq.sum) +
  tm_dots(col = "res", size=2)
```

:::{.notes}
Band of strong red/green colors indicate part of the state where the X/Y model isn't doing well
:::

## Calculate the experimental variogram

* __nugget__ - the proportion of semivariance that occurs at small distances

* __sill__ - the maximum semivariance between pairs of observations

* __range__ - the distance at which the __sill__ occurs 

* __experimental__ vs. __fitted__ variograms

:::{.notes}
Variogram - how variation changes over distance
semivariogram - all positive (absolute value of residuals)
:::

## A Note about Semivariograms

![](img/slide_16/index.png)

:::{.notes}
Sill - things can only get so weird

Connect to range in Ripley's K
:::

## Fitted Semivariograms

* Rely on functional forms to model semivariance

![](img/slide_16/Variogram-models.png)

## Calculate the experimental variogram

```{r}
#| echo: true
var.cld  <- gstat::variogram(res ~ 1, aq.sum, cloud = TRUE)
var.df  <- as.data.frame(var.cld)
index1  <- which(with(var.df, left==21 & right==2))
```
## Calculate the experimental variogram

```{r}
#| echo: true
OP <- par( mar=c(4,6,1,1))
plot(var.cld$dist/1000 , var.cld$gamma, col="grey", 
     xlab = "Distance between point pairs (km)",
     ylab = expression( frac((res[2] - res[1])^2 , 2)) )
par(OP)
```

## Simplifying the cloud plot

```{r}
#| echo: true
# Compute the sample experimental variogram
var.smpl <- gstat::variogram(f.2, aq.sum, cloud = FALSE)

bins.ct <- c(0, var.smpl$dist , max(var.cld$dist) )
bins <- vector()
for (i in 1: (length(bins.ct) - 1) ){
  bins[i] <- mean(bins.ct[ seq(i,i+1, length.out=2)] ) 
}
bins[length(bins)] <- max(var.cld$dist)
var.bins <- findInterval(var.cld$dist, bins)
```

## Simplifying the cloud plot

```{r}
#| echo: true
# Point data cloud with bin boundaries
OP <- par( mar = c(5,6,1,1))
plot(var.cld$gamma ~ eval(var.cld$dist/1000), col=rgb(0,0,0,0.2), pch=16, cex=0.7,
     xlab = "Distance between point pairs (km)",
     ylab = expression( gamma ) )
points( var.smpl$dist/1000, var.smpl$gamma, pch=21, col="black", bg="red", cex=1.3)
abline(v=bins/1000, col="red", lty=2)
par(OP)
```

<!--
```{r}

library(stringr)
library(lattice)
kk <- str_extract_all(gstat::vgm()$long, "\\([^()]+\\)")
kk <- substring(kk, 2, nchar(kk)-1)
gstat::show.vgms(strip=strip.custom(factor.levels=kk))
```
-->

## Looking at the sample Variogram

```{r}
# Compute the sample variogram, note the f.2 trend model is one of the parameters
# passed to variogram(). This tells the function to create the variogram on
# the de-trended data
var.smpl <- gstat::variogram(f.2, aq.sum, cloud = FALSE, cutoff = 1000000, width = 89900)


# Compute the variogram model by passing the nugget, sill and range values
# to fit.variogram() via the vgm() function.
dat.fit  <- gstat::fit.variogram(var.smpl, gstat::vgm(nugget = 12, range= 60000, model="Gau", cutoff=1000000))

# The following plot allows us to gauge the fit
plot(var.smpl, dat.fit)
```
## Estimating the sample variogram

```{r}
#| echo: true
#| eval: false
# Compute the sample variogram, note the f.2 trend model is one of the parameters
# passed to variogram(). This tells the function to create the variogram on
# the de-trended data
var.smpl <- gstat::variogram(f.2, aq.sum, cloud = FALSE, cutoff = 1000000, width = 89900)


# Compute the variogram model by passing the nugget, sill and range values
# to fit.variogram() via the vgm() function.
dat.fit  <- gstat::fit.variogram(var.smpl, gstat::vgm(nugget = 12, range= 60000, model="Gau", cutoff=1000000))

# The following plot allows us to gauge the fit
plot(var.smpl, dat.fit)

```

## Ordinary Kriging

```{r}
dat.krg <- gstat::krige( formula = res~1, 
                         locations = aq.sum, 
                         newdata = grd[, c("X", "Y", "var2.pred")], 
                         model = dat.fit)

dat.krg.preds <-  dat.krg %>%
  mutate(X = st_coordinates(.)[, 1], 
         Y = st_coordinates(.)[, 2]) %>%
  select(X, Y, var1.pred) %>%
  st_drop_geometry()

r <- rast(dat.krg.preds, crs = crs(grd))
r.m <- mask(r, st_as_sf(id.cty))

# Plot the raster and the sampled points
tm_shape(r.m) + 
  tm_raster(n=10, palette="RdBu", title="Predicted residual \nair quality") +
  tm_shape(aq.sum) + tm_dots(size=0.2) +
  tm_legend(legend.outside=TRUE)

```

## Ordinary Kriging

```{r}
#| echo: true
#| eval: false
dat.krg <- gstat::krige( formula = res~1, 
                         locations = aq.sum, 
                         newdata = grd[, c("X", "Y", "var2.pred")], 
                         model = dat.fit)
```

## Combining with the trend data

```{r}
dat.krg <- gstat::krige( formula = f.2, 
                         locations = aq.sum, 
                         newdata = grd[, c("X", "Y", "var2.pred")], 
                         model = dat.fit)

dat.krg.preds <-  dat.krg %>%
  mutate(X = st_coordinates(.)[, 1], 
         Y = st_coordinates(.)[, 2]) %>%
  select(X, Y, var1.pred) %>%
  st_drop_geometry()

r <- rast(dat.krg.preds, crs = crs(grd))
r.m <- mask(r, st_as_sf(id.cty))

# Plot the raster and the sampled points
tm_shape(r.m) + tm_raster(n=10, title="Predicted air quality") +tm_shape(aq.sum) + tm_dots(size=0.2) +
  tm_legend(legend.outside=TRUE)

```

## Combining with the trend data

```{r}
#| echo: true
#| eval: false
dat.krg <- gstat::krige( formula = f.2, 
                         locations = aq.sum, 
                         newdata = grd[, c("X", "Y", "var2.pred")], 
                         model = dat.fit)

dat.krg.preds <-  dat.krg %>%
  mutate(X = st_coordinates(.)[, 1], 
         Y = st_coordinates(.)[, 2]) %>%
  select(X, Y, var1.pred) %>%
  st_drop_geometry()

r <- rast(dat.krg.preds, crs = crs(grd))
r.m <- mask(r, st_as_sf(id.cty))

# Plot the raster and the sampled points
tm_shape(r.m) + tm_raster(n=10, title="Predicted air quality") +tm_shape(aq.sum) + tm_dots(size=0.2) +
  tm_legend(legend.outside=TRUE)

```

## Visualizing Uncertainty

```{r}

# The dat.krg object stores not just the interpolated values, but the 
# variance values as well. These can be passed to the raster object
# instead of the interpolated values as follows
dat.krg.var <-  dat.krg %>%
  mutate(X = st_coordinates(.)[, 1], 
         Y = st_coordinates(.)[, 2]) %>%
  select(X, Y, var1.var) %>%
  st_drop_geometry()

r <- rast(dat.krg.var, crs = crs(grd))
r.m <- mask(r, st_as_sf(id.cty))

tm_shape(r.m) + 
  tm_raster(n=7, palette ="Reds", ,title="Variance map ") +
  tm_shape(aq.sum) + tm_dots(size=0.2) +
  tm_legend(legend.outside=TRUE)
```

:::{.notes}
Point out edge effects
:::

<!--

## Universal Kriging

* Assumes that the deterministic part of the process ($\mu(\mathbf{x})$) is now a function of the location $\mathbf{x}$

* Could be the location or some other attribute

* Now `y` is a function of some aspect of `x`

```{r}
#| echo: true
#| eval: false

vu <- variogram(log(zinc)~elev, ~x+y, data=meuse)
mu <- fit.variogram(vu, vgm(1, "Sph", 300, 1))
gUK <- gstat(NULL, "log.zinc", log(zinc)~elev, meuse, locations=~x+y, model=mu)
names(r) <- "elev"
UK <- interpolate(r, gUK, debug.level=0)
```

## Universal Kriging

```{r}
#| eval: false
UK.msk <- mask(UK, r)
plot(UK.msk)
```

## Universal Kriging

```{r}
#| echo: true
#| eval: false
vu <- variogram(log(zinc)~x + x^2 + y + y^2, ~x+y, data=meuse)
mu <- fit.variogram(vu, vgm(1, "Sph", 300, 1))
gUK <- gstat(NULL, "log.zinc", log(zinc)~x + x^2 + y + y^2, meuse, locations=~x+y, model=mu)
names(r) <- "elev"
UK <- interpolate(r, gUK, debug.level=0)
```

## Universal Kriging

```{r}
#| eval: false
UK.msk <- mask(UK, r)
plot(UK.msk)
```

## Co-Kriging

* relies on autocorrelation in $\epsilon_1(\mathbf{x})$ for $z_1$ AND cross correlation with other variables ($z_{2...j}$)

* Extending the ordinary kriging model gives:

$$
\begin{equation}
z_1(\mathbf{x}) = \mu_1 + \epsilon_1(\mathbf{x})\\
z_2(\mathbf{x}) = \mu_2 + \epsilon_2(\mathbf{x})
\end{equation}
$$
* Note that there is autocorrelation within both $z_1$ and $z_2$ (because of the $\epsilon$) and cross-correlation (because of the location, $\mathbf{x}$)

* Not required that all variables are measured at exactly the same points

## Co-Kriging

* Process is just a linked series of `gstat` calls

```{r}
#| echo: true
#| eval: false
gCoK <- gstat(NULL, 'log.zinc', log(zinc)~1, meuse, locations=~x+y)
gCoK <- gstat(gCoK, 'elev', elev~1, meuse, locations=~x+y)
gCoK <- gstat(gCoK, 'cadmium', cadmium~1, meuse, locations=~x+y)
coV <- variogram(gCoK)
coV.fit <- fit.lmc(coV, gCoK, vgm(model='Sph', range=1000))

coK <- interpolate(r, coV.fit, debug.level=0)

```

## Co-Kriging

```{r}
#| eval: false
plot(coV, coV.fit, main='Fitted Co-variogram')
```

## Co-Kriging

```{r}
#| eval: false
coK.msk <- mask(coK, r)
plot(coK.msk)
```

## A Note about Semivariograms


-->