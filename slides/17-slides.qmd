---
title: "Point Pattern Analysis"
subtitle: "HES 505 Fall 2024: Session 17"
author: "Carolyn Koehn"
format: 
  revealjs:
    theme: mytheme.scss
    slide-number: true
    show-slide-number: print
    self-contained: true  
---

```{r}
#| include: false
library(spatstat)
```

# Objectives {background="#9F281A"}

- Define a point process and their utility for ecological applications

- Define first and second-order Complete Spatial Randomness

- Use several common functions to explore point patterns

- Leverage point patterns to interpolate missing data

## What is a point pattern?

::: columns
::: {.column width="60%"}
::: {style="font-size: 0.7em"}
* _Point pattern_: A __set__ of __events__ within a study region (i.e., a _window_) generated by a random process

* __Set__: A collection of mathematical __events__

* __Events__: The existence of a point object of the type we are interested in at a particular location in the study region

* A _marked point pattern_ refers to a point pattern where the events have additional descriptors

:::
:::
::: {.column width="40%"}
::: {style="font-size: 0.7em"}
__Some notation:__

* $S$: refers to the entire set

* $s_i$ refers to one event (point) in set $S$

* $\mathbf{s_i}$ denotes the vector of data describing point $s_i$ in set $S$

* $\#(S \in A )$ refers to the number of points in $S$ within study area $A$
:::
:::
:::

## Requirements for a set to be considered a point pattern

* The pattern must be mapped on a plane to preserve distance

* The study area, $A$, should be objectively determined

* There should be a $1:1$ correspondence between objects in $A$ and events in the pattern (no undetected points)

* Events must be _proper_ i.e., refer to actual locations of the event

::: {.notes}
Question: What does mapping on a plane have to do with the CRS you choose?
:::

## Describing Point Patterns

::: columns
::: {.column width="60%"}
::: {style="font-size: 0.7em"}

* _Density-based metrics_: the $\#$ of points within area, $a$, in study area $A$

* _Distance-based metrics_: based on nearest neighbor distances or the distance matrix for all points

* _First order_ effects reflect variation in __intensity__ due to variation in the 'attractiveness' of locations

* _Second order_ effects reflect variation in __intensity__ due to the presence of points themselves
:::
:::
::: {.column width="40%"}
![from Manuel Gimond](img/slide_16/1st_2nd_order_property.png)
:::
:::

::: {.notes}
- First order effects: on-the-ground variation affects distribution (e.g. good habitat vs garbage habitat)

- Second order effects: points themselves effect occurance of other points (e.g. flocking birds, nurse plants)
:::

## Centrography
::: columns
::: {.column width="40%"}
::: {style="font-size: 0.7em"}
* _Mean center_: the point, $\hat{\mathbf{s}}$, whose coordinates are the average of all events in the pattern
* _Standard distance_: a measure of the dispersion of points around the _mean center_
* _Standard ellipse_: dispersion in one dimension
:::
:::
::: {.column width="60%"}
![From Manuel Gimond](img/slide_16/centrography.png) 
:::
:::

::: {.notes}
- Analogous to summary statistics for standard data (mean, sd, interquartile range) but for space
- These are the sort of things that show up in the first paragraph of the results -- a sense for how dispersed the events are
:::

## Analyzing Point Patterns

* Modeling random processes means we are interested in probability densities of the points (first-order;density)

* Also interested in how the presence of some events affects the probability of other events (second-order;distance)

* Finally interested in how the attributes of an event affect location (marked)

* Need to introduce a few new packages (`spatstat` and `gstat`)

::: {.notes}
- First, are these events just randomly distributed? If they are, our covariates probably don't matter...
:::

## Density based methods

::: columns
::: {.column width="60%"}

*   The overall _intensity_ of a point pattern is a crude density estimate

$$
\begin{equation}
\hat{\lambda} = \frac{\#(S \in A )}{a}
\end{equation}
$$
* Local density = quadrat counts

:::
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 5
nclust <- function(x0, y0, radius, n) {
              return(runifdisc(n, radius, centre=c(x0, y0)))
            }
x <- rpoispp(lambda =50)
Q = quadratcount(x)
plot(x, main="")
plot(Q, add=TRUE)
```
:::
:::

::: {.notes}
Lambda is a rate, this is a quadrat
:::


# Analyzing Point Patterns {background="#9F281A"}

## Kernel Density Estimates (KDE)

$$
\begin{equation}
\hat{f}(x) = \frac{1}{nh_xh_y} \sum_{i=1}^n k\bigg(\frac{{x-x_i}}{h_x},\frac{{y-y_i}}{h_y} \bigg)
\end{equation}
$$

::: {style="font-size: 0.7em"}
* Assume each location in $\mathbf{s_i}$ drawn from unknown distribution

* Distribution has probability density $f(\mathbf{x})$

* Estimate $f(\mathbf{x})$ by averaging probability "bumps" around each location

:::

::: {.notes}
- Quadrats are arbitrary. When we want to measure continuous density changes, these choices start to matter
- Model the shape of intensity
- This is a normal kernal -- hill that tapers off in all directions equally
:::

## Kernel Density Estimates (KDE)
::: {style="font-size: 0.7em"}
* $h$ is the bandwidth and $k$ is the kernel

* We can use `stats::density` to explore

* __kernel__: defines the shape, size, and weight assigned to observations in the window

* __bandwidth__ often assigned based on distance from the window center
:::

::: {.notes}
- A density plot is a smooth of your histogram
- Bandwidth: how far to look for other points (small bandwidth = rougher surface)
:::

## Kernel Density Estimates in Action

```{r}
#| echo: true
#| warning: false

library(spatstat)

x <- rpoispp(lambda =50)
K0 <- density(x)
K1 <- density(x, adjust=0.5)
K2 <- density(x, adjust=1.5)
K3 <- density(x, kernel="disc")
```
```{r}
par(mfrow=c(2,2))
plot(K0, main="Bandwidth=default,\nkernel=default (gaussian)")
plot(K3, main="Bandwidth=default,\nkernel=disc")
plot(K1, main="Bandwidth=default * 0.5")
plot(K2, main="Bandwidth=default * 1.5")
par(mfrow=c(1,1))
```

## Choosing bandwidths and kernels

* Small values for $h$ give 'spiky' densities

* Large values for $h$ smooth much more

* Some kernels have optimal bandwidth detection

* `tmap` package provides additional functionality

::: {.notes}
- No covariates here!
:::

# Second-Order Analysis {background="#9F281A"}

## Second-Order Analysis

* KDEs assume independence of points (first order randomness)

* Second-order methods allow dependence among points (second-order randomness)

* Several functions for assessing second order dependence ($K$, $L$, and $G$)

## Distance based metrics

* Provide an estimate of the _second order_ effects

* _Mean nearest-neighbor distance_:
$$\hat{d}_{min} = \frac{\sum_{i = 1}^{m} d_{min}(\mathbf{s_i})}{n}$$

::: {.notes}
All metrics are based in neighbor distances
:::

## Nearest-neighbor distance

```{r}
#| echo: true

ANN <- apply(nndist(x, k=1:50),2,FUN=mean)
plot(ANN ~ eval(1:50), type="b", main=NULL, las=1, 
     xlab="kth nearest neighbor", ylab="Mean distance")
```

::: {.notes}
Not very good for recognizing clumping, which is what we're interested in with second order effects
:::

## Ripley's $K$ Function

::: {style="font-size: 0.7em"}
* Nearest neighbor methods throw away a lot of information

* If points have independent, fixed marginal densities, then they exhibit _complete, spatial randomness_ (CSR)

* The _K_ function is an alternative, based on a series of circles with increasing radius

$$
\begin{equation}
K(d) = \lambda^{-1}E(N_d)
\end{equation}
$$

* We can test for clustering by comparing to the expectation:

$$
\begin{equation}
K_{CSR}(d) = \pi d^2
\end{equation}
$$

* if $k(d) > K_{CSR}(d)$ then there is clustering at the scale defined by $d$

:::

::: {.notes}
- Increasing radius: at which distances might we see some clustering
- K is the rate (which we may not entirely know) times how many points you have within a given distance
- "Null" hypothesis is area of a circle. When K diverges, we have more or less points than we expected given the distance. Actually making lambda (rate) equal 1.
:::

## Ripley's $K$ Function

* When working with a sample the distribution of $K$ is unknown

* Estimate with

$$
\begin{equation}
\hat{K}(d) = \hat{\lambda}^{-1}\sum_{i=1}^n\sum_{j=1}^n\frac{I(d_{ij} <d)}{n(n-1)}
\end{equation}
$$

where:

$$
\begin{equation}
\hat{\lambda} = \frac{n}{|A|}
\end{equation}
$$

::: {.notes}
First slide is ideal world (simulation). This is estimation with data. R will do it for you!
:::

## Ripley's $K$ Function

Using the `spatstat` package:

```{r}
#| fig-width: 4
#| fig-height: 4
data(bramblecanes)
plot(bramblecanes)
```


## Ripley's $K$ Function

```{r}
#| echo: true
kf <- Kest(bramblecanes, correction-"border")
plot(kf)
```

::: {.notes}
- Different K-hats refer to different assumptions about lambda and its shape
- Radius on x-axis, K on y axis
- What are we missing to tell us if our data is clustered?
:::

## Ripley's $K$ Function
* accounting for variation in $d$

```{r}
#| echo: true
kf.env <- envelope(bramblecanes, correction="border", envelope = FALSE, verbose = FALSE)
plot(kf.env)
```

::: {.notes}
- Red line is expectation under complete spatial randomness
- We need to include a confidence region since lines will rarely overlap entirely, so we simulate point patterns under CSR (these are still stochastic processes even if totally random)
- Is there clustering? Range of clustering shown - scale of analysis matters. What kind of distance is necessary to overcome spatial autocorrelation?
:::

## Other functions

::: columns
::: {.column width="60%"}

* $L$ function: square root transformation of $K$

* $G$ function: the cummulative frequency distribution of the nearest neighbor distances

* $F$ function: similar to $G$ but based on randomly located points


:::
::: {.column width="40%"}

```{r}
#| fig-width: 5
#| fig-height: 5
gf.env <- envelope(bramblecanes, Gest, correction="border", verbose = FALSE)
plot(gf.env)
```

:::
:::

::: {.notes}
G lets you compare the marks of the points -- beyond locations to attributes at locations. E.g. is productivity of eagle nests clustered?
:::

